#!/usr/bin/env python3
"""
Local Processor - Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ollama Ğ¸ MLX-Whisper
================================================================
ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° PDF Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ñ… API.

Ğ¢Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:
- Ollama Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ llama3.3 (Ñ‚ĞµĞºÑÑ‚) Ğ¸ llava:34b (vision)
- mlx-whisper Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    # PDF Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ LLM
    python local_processor.py pdf Ñ„Ğ°Ğ¹Ğ».pdf --mode full_analysis

    # PDF Vision Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ LLM
    python local_processor.py vision ÑĞºĞ°Ğ½.pdf --mode extract_text

    # ĞÑƒĞ´Ğ¸Ğ¾ Ñ‡ĞµÑ€ĞµĞ· MLX-Whisper
    python local_processor.py audio Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ.mp3
"""

import os
import sys
import json
import base64
import hashlib
import subprocess
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from io import BytesIO
import argparse
import requests

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹
sys.path.insert(0, str(Path(__file__).parent))
from utils import (
    get_logger, load_config, load_env_file,
    OUTPUT_DIR, TRANSCRIPTS_DIR, ensure_directories
)

# PDF processing
try:
    import pdfplumber
    from pypdf import PdfReader
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

# PDF to image
try:
    from pdf2image import convert_from_path
    from PIL import Image
    PDF2IMAGE_AVAILABLE = True
except ImportError:
    PDF2IMAGE_AVAILABLE = False

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
load_env_file()
ensure_directories()
logger = get_logger(__name__)

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
DEFAULT_CONFIG = {
    "ollama_base_url": "http://localhost:11434",
    "text_model": "llama3.3",           # Ğ”Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°
    "vision_model": "llava:34b",         # Ğ”Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹
    "whisper_model": "large-v3",         # MLX-Whisper Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    "max_tokens_per_chunk": 4000,
    "dpi": 150,
    "language": "ru"
}


class OllamaClient:
    """ĞšĞ»Ğ¸ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ollama API."""

    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self._check_connection()

    def _check_connection(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº Ollama."""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            response.raise_for_status()
        except requests.exceptions.ConnectionError:
            raise ConnectionError(
                "ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğº Ollama. "
                "Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ollama Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½: ollama serve"
            )

    def list_models(self) -> List[str]:
        """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."""
        response = requests.get(f"{self.base_url}/api/tags")
        data = response.json()
        return [m["name"] for m in data.get("models", [])]

    def generate(self, model: str, prompt: str, system: str = None,
                 images: List[str] = None, stream: bool = False) -> str:
        """
        Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.

        Args:
            model: Ğ˜Ğ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            prompt: ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
            system: Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
            images: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº base64 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (Ğ´Ğ»Ñ vision Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹)
            stream: Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
        """
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "num_predict": 4096,
                "temperature": 0.7
            }
        }

        if system:
            payload["system"] = system

        if images:
            payload["images"] = images

        response = requests.post(
            f"{self.base_url}/api/generate",
            json=payload,
            timeout=300  # 5 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚
        )
        response.raise_for_status()

        return response.json()["response"]

    def chat(self, model: str, messages: List[Dict],
             images: List[str] = None) -> str:
        """
        Chat completion Ñ‡ĞµÑ€ĞµĞ· Ollama.

        Args:
            model: Ğ˜Ğ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            messages: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ [{"role": "user/assistant", "content": "..."}]
            images: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº base64 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹
        """
        payload = {
            "model": model,
            "messages": messages,
            "stream": False,
            "options": {
                "num_predict": 4096,
                "temperature": 0.7
            }
        }

        # Ğ”Ğ»Ñ vision Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ
        if images and messages:
            messages[-1]["images"] = images

        response = requests.post(
            f"{self.base_url}/api/chat",
            json=payload,
            timeout=300
        )
        response.raise_for_status()

        return response.json()["message"]["content"]


class LocalPDFProcessor:
    """Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° PDF Ñ‡ĞµÑ€ĞµĞ· Ollama."""

    def __init__(self, config: dict = None):
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°."""
        self.config = {**DEFAULT_CONFIG, **(config or {})}
        self.client = OllamaClient(self.config["ollama_base_url"])
        self.output_dir = OUTPUT_DIR

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        available_models = self.client.list_models()
        if self.config["text_model"] not in available_models:
            logger.warning(
                f"ĞœĞ¾Ğ´ĞµĞ»ÑŒ {self.config['text_model']} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. "
                f"Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ: {available_models}"
            )

        logger.info(f"LocalPDFProcessor Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {self.config['text_model']}")

    def extract_text_from_pdf(self, pdf_path: str) -> Tuple[str, Dict]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· PDF."""
        if not PDFPLUMBER_AVAILABLE:
            raise ImportError("Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: pip install pdfplumber pypdf")

        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {pdf_path}")

        metadata = {
            "filename": pdf_path.name,
            "path": str(pdf_path),
            "extracted_at": datetime.now().isoformat(),
            "pages": 0,
            "characters": 0
        }

        full_text = []

        print(f"ğŸ“„ Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ·: {pdf_path.name}")

        try:
            with pdfplumber.open(pdf_path) as pdf:
                metadata["pages"] = len(pdf.pages)

                for i, page in enumerate(pdf.pages, 1):
                    print(f"  Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {i}/{metadata['pages']}...", end="\r")

                    text = page.extract_text() or ""
                    tables = page.extract_tables()

                    page_content = f"\n\n--- Ğ¡Ğ¢Ğ ĞĞĞ˜Ğ¦Ğ {i} ---\n\n{text}"

                    if tables:
                        page_content += "\n\n[Ğ¢ĞĞ‘Ğ›Ğ˜Ğ¦Ğ«]\n"
                        for j, table in enumerate(tables, 1):
                            page_content += f"\nĞ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° {j}:\n"
                            for row in table:
                                if row:
                                    page_content += " | ".join(str(cell) if cell else "" for cell in row) + "\n"

                    full_text.append(page_content)

        except Exception as e:
            # Fallback to pypdf
            reader = PdfReader(pdf_path)
            metadata["pages"] = len(reader.pages)
            for i, page in enumerate(reader.pages, 1):
                text = page.extract_text() or ""
                full_text.append(f"\n\n--- Ğ¡Ğ¢Ğ ĞĞĞ˜Ğ¦Ğ {i} ---\n\n{text}")

        result_text = "\n".join(full_text)
        metadata["characters"] = len(result_text)

        print(f"\nâœ… Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¾ {metadata['pages']} ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, {metadata['characters']:,} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²")

        return result_text, metadata

    def smart_chunk_text(self, text: str, max_chars: int = None) -> List[Dict]:
        """Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸."""
        if max_chars is None:
            max_chars = self.config["max_tokens_per_chunk"] * 4

        import re

        chunks = []
        separators = [
            r'\n\n--- Ğ¡Ğ¢Ğ ĞĞĞ˜Ğ¦Ğ \d+ ---\n\n',
            r'\n\n#{1,3} ',
            r'\n\n',
            r'\n',
            r'\. ',
        ]

        current_pos = 0
        chunk_num = 0

        while current_pos < len(text):
            chunk_end = min(current_pos + max_chars, len(text))

            if chunk_end < len(text):
                search_start = max(current_pos + max_chars // 2, current_pos)
                search_text = text[search_start:chunk_end]

                best_break = None
                for sep in separators:
                    matches = list(re.finditer(sep, search_text))
                    if matches:
                        best_break = search_start + matches[-1].end()
                        break

                if best_break:
                    chunk_end = best_break

            chunk_text = text[current_pos:chunk_end].strip()

            if chunk_text:
                chunk_num += 1
                page_matches = re.findall(r'--- Ğ¡Ğ¢Ğ ĞĞĞ˜Ğ¦Ğ (\d+) ---', chunk_text)
                pages = [int(p) for p in page_matches] if page_matches else []

                chunks.append({
                    "chunk_id": chunk_num,
                    "text": chunk_text,
                    "char_count": len(chunk_text),
                    "pages": pages,
                    "page_range": f"{min(pages)}-{max(pages)}" if pages else "N/A"
                })

            current_pos = chunk_end

        print(f"ğŸ“¦ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ {len(chunks)} Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²")
        return chunks

    def get_system_prompt(self, mode: str) -> str:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚."""
        base = """Ğ¢Ñ‹ - AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Executive Coaching Ğ² Ğ¡ĞšĞĞ›ĞšĞĞ’Ğ.
Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° - Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ÑƒÑ‡ĞµĞ±Ğ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ» Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ†ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ.

ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢ ĞŸĞ ĞĞ“Ğ ĞĞœĞœĞ«:
- ĞŸÑ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ° Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ĞºĞ¾ÑƒÑ‡Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹
- Ğ¤Ğ¾ĞºÑƒÑ Ğ½Ğ° executive-ĞºĞ¾ÑƒÑ‡Ğ¸Ğ½Ğ³
- Ğ Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ»Ğ¸Ğ´ĞµÑ€ÑÑ‚Ğ²Ğ°

ĞŸĞ Ğ˜ĞĞ¦Ğ˜ĞŸĞ«:
1. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞ¹ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ
2. Ğ’Ñ‹Ğ´ĞµĞ»ÑĞ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
3. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ

"""

        mode_prompts = {
            "summary": """Ğ—ĞĞ”ĞĞ§Ğ: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸

## ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚ĞµĞ¼Ñ‹
## ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ´ĞµĞ¸
## Ğ’Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ñ‹/Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ
""",
            "key_concepts": """Ğ—ĞĞ”ĞĞ§Ğ: Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸

## ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸
| ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ | ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ | ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ |

## ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸
## Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
""",
            "coaching_tools": """Ğ—ĞĞ”ĞĞ§Ğ: Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾ÑƒÑ‡Ğ¸Ğ½Ğ³Ğ°

## Ğ¢ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ¶Ğ½ĞµĞ½Ğ¸Ñ
## ĞœĞ¾Ñ‰Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹
## ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
""",
            "full_analysis": """Ğ—ĞĞ”ĞĞ§Ğ: ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°

## 1. Ğ¡ĞĞœĞœĞĞ Ğ˜
## 2. ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• ĞšĞĞĞ¦Ğ•ĞŸĞ¦Ğ˜Ğ˜
## 3. Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞœĞ•ĞĞ¢Ğ« Ğ˜ Ğ¢Ğ•Ğ¥ĞĞ˜ĞšĞ˜
## 4. ĞœĞĞ©ĞĞ«Ğ• Ğ’ĞĞŸĞ ĞĞ¡Ğ«
## 5. Ğ¢Ğ•Ğ“Ğ˜ Ğ”Ğ›Ğ¯ ĞŸĞĞ˜Ğ¡ĞšĞ
"""
        }

        return base + mode_prompts.get(mode, mode_prompts["summary"])

    def process_chunk(self, chunk: Dict, mode: str = "full_analysis") -> Dict:
        """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ¸Ğ½ Ñ‡Ğ°Ğ½Ğº Ñ‡ĞµÑ€ĞµĞ· Ollama."""
        system_prompt = self.get_system_prompt(mode)

        user_message = f"""ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ¹ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°:

---
{chunk['text']}
---

Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹: {chunk.get('page_range', 'N/A')}
"""

        try:
            print(f"   ğŸ”„ ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ğ² {self.config['text_model']}...")

            response = self.client.generate(
                model=self.config["text_model"],
                prompt=user_message,
                system=system_prompt
            )

            return {
                "chunk_id": chunk["chunk_id"],
                "page_range": chunk.get("page_range", "N/A"),
                "mode": mode,
                "processed_at": datetime.now().isoformat(),
                "response": response,
                "status": "success",
                "model": self.config["text_model"]
            }

        except Exception as e:
            return {
                "chunk_id": chunk["chunk_id"],
                "page_range": chunk.get("page_range", "N/A"),
                "mode": mode,
                "processed_at": datetime.now().isoformat(),
                "error": str(e),
                "status": "error"
            }

    def process_pdf(self, pdf_path: str, mode: str = "full_analysis") -> Dict:
        """ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ PDF."""
        pdf_path = Path(pdf_path)

        pdf_hash = hashlib.md5(pdf_path.name.encode()).hexdigest()[:8]
        session_id = f"{pdf_path.stem}_local_{pdf_hash}"

        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     ğŸ  Local PDF Processor (Ollama)                          â•‘
â•‘     ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {self.config['text_model']:<44}â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚
        text, metadata = self.extract_text_from_pdf(pdf_path)

        # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸
        chunks = self.smart_chunk_text(text)

        # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼
        results = []
        total = len(chunks)

        print(f"\nğŸš€ ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ {total} Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ '{mode}'")
        print("=" * 50)

        for i, chunk in enumerate(chunks, 1):
            print(f"\nğŸ“ Ğ§Ğ°Ğ½Ğº {i}/{total} (ÑÑ‚Ñ€. {chunk.get('page_range', 'N/A')})...")
            result = self.process_chunk(chunk, mode)
            results.append(result)

            if result["status"] == "success":
                print(f"   âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾")
            else:
                print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {result.get('error')}")

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
        final_result = {
            "session_id": session_id,
            "source_pdf": str(pdf_path),
            "metadata": metadata,
            "mode": mode,
            "processor": "local_ollama",
            "model": self.config["text_model"],
            "total_chunks": total,
            "successful_chunks": sum(1 for r in results if r["status"] == "success"),
            "processed_at": datetime.now().isoformat(),
            "results": results
        }

        output_json = self.output_dir / f"{session_id}_processed.json"
        with open(output_json, "w", encoding="utf-8") as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)

        output_md = self.output_dir / f"{session_id}_processed.md"
        self._save_as_markdown(final_result, output_md)

        print("\n" + "=" * 50)
        print(f"âœ… ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°!")
        print(f"ğŸ“Š Ğ§Ğ°Ğ½ĞºĞ¾Ğ²: {final_result['successful_chunks']}/{total}")
        print(f"ğŸ“ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹: {output_md}")

        return final_result

    def _save_as_markdown(self, result: Dict, output_path: Path):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ² Markdown."""
        md = f"""# ğŸ  Local Analysis: {Path(result['source_pdf']).name}

**ĞœĞ¾Ğ´ĞµĞ»ÑŒ:** {result.get('model', 'N/A')}
**Ğ”Ğ°Ñ‚Ğ°:** {result['processed_at']}
**Ğ ĞµĞ¶Ğ¸Ğ¼:** {result['mode']}
**Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†:** {result['metadata'].get('pages', 'N/A')}

---

"""
        for r in result["results"]:
            if r["status"] == "success":
                md += f"""
## Ğ¤Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚ {r['chunk_id']} (ÑÑ‚Ñ€. {r['page_range']})

{r['response']}

---
"""

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(md)


class LocalVisionProcessor:
    """Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° PDF Ñ‡ĞµÑ€ĞµĞ· Ollama Vision."""

    def __init__(self, config: dict = None):
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°."""
        if not PDF2IMAGE_AVAILABLE:
            raise ImportError("Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: pip install pdf2image pillow")

        self.config = {**DEFAULT_CONFIG, **(config or {})}
        self.client = OllamaClient(self.config["ollama_base_url"])
        self.output_dir = OUTPUT_DIR

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ vision Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        available_models = self.client.list_models()
        if self.config["vision_model"] not in available_models:
            # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹
            vision_alternatives = ["llava:34b", "llava:13b", "llava:7b", "llava"]
            for alt in vision_alternatives:
                if alt in available_models:
                    self.config["vision_model"] = alt
                    break
            else:
                logger.warning(
                    f"Vision Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°. Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ: {available_models}. "
                    "Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: ollama pull llava:34b"
                )

        logger.info(f"LocalVisionProcessor Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {self.config['vision_model']}")

    def pdf_to_images(self, pdf_path: str, dpi: int = None) -> List[Image.Image]:
        """ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ PDF Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."""
        dpi = dpi or self.config["dpi"]
        pdf_path = Path(pdf_path)

        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {pdf_path}")

        print(f"ğŸ“„ ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒÑ PDF: {pdf_path.name} (DPI: {dpi})")

        images = convert_from_path(str(pdf_path), dpi=dpi)

        print(f"   âœ… Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ {len(images)} ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†")

        return images

    def image_to_base64(self, image: Image.Image, max_size: int = 1024) -> str:
        """ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² base64."""
        # Ğ ĞµÑĞ°Ğ¹Ğ·Ğ¸Ğ¼ Ğ´Ğ»Ñ Ollama
        if max(image.size) > max_size:
            ratio = max_size / max(image.size)
            new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
            image = image.resize(new_size, Image.Resampling.LANCZOS)

        buffer = BytesIO()
        image.save(buffer, format="PNG")
        return base64.standard_b64encode(buffer.getvalue()).decode("utf-8")

    def get_system_prompt(self, mode: str) -> str:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ vision/OCR."""
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼, ÑÑ‚Ğ¾ OCR-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ»Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ
        model_name = self.config.get('vision_model', '').lower()
        is_ocr_model = any(kw in model_name for kw in [
            'granite', 'qwen2.5vl', 'qwen2-vl', 'deepseek-ocr', 'minicpm'
        ])

        if is_ocr_model:
            # Ğ”Ğ»Ñ OCR-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¿Ñ€ÑĞ¼Ğ°Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°
            base = """Ğ¢Ñ‹ â€” OCR-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°. Ğ¢Ğ²Ğ¾Ñ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° â€” Ğ”ĞĞ¡Ğ›ĞĞ’ĞĞ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ²ĞµÑÑŒ Ñ‚ĞµĞºÑÑ‚ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.

ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜ Ğ’ĞĞ–ĞĞ:
- ĞĞ• Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
- ĞĞ• Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞ¹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ
- Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ğ¹ Ğ¸ ĞºĞ¾Ğ¿Ğ¸Ñ€ÑƒĞ¹ Ñ‚ĞµĞºÑÑ‚ ĞšĞĞš Ğ•Ğ¡Ğ¢Ğ¬
- Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞ¹ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ñ‚ĞµĞºÑÑ‚Ğ° (Ñ€ÑƒÑÑĞºĞ¸Ğ¹, Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ Ğ¸ Ñ‚.Ğ´.)
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ markdown Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸, ÑĞ¿Ğ¸ÑĞºĞ¸, Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹)

"""
        else:
            # Ğ”Ğ»Ñ llava Ğ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ…: Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸
            base = """Ğ¢Ñ‹ - AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Executive Coaching Ğ² Ğ¡ĞšĞĞ›ĞšĞĞ’Ğ.
Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° â€” Ğ˜Ğ—Ğ’Ğ›Ğ•Ğ§Ğ¬ Ğ¢Ğ•ĞšĞ¡Ğ¢ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° ĞĞ• Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºÑƒ.

Ğ’ĞĞ–ĞĞ:
1. Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°Ğ¹ Ğ’Ğ•Ğ¡Ğ¬ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğ¹ Ğ¢Ğ•ĞšĞ¡Ğ¢ Ğ´Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ğ¾
2. ĞĞ• Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ¹ Ñ‡Ñ‚Ğ¾ Ñ‚Ñ‹ Ğ²Ğ¸Ğ´Ğ¸ÑˆÑŒ ("Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾...")
3. ĞŸĞ ĞĞ¡Ğ¢Ğ ĞºĞ¾Ğ¿Ğ¸Ñ€ÑƒĞ¹ Ñ‚ĞµĞºÑÑ‚ ĞºĞ°Ğº Ğ¾Ğ½ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½
4. Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞ¹ Ğ² markdown
5. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞ¹ ÑĞ·Ñ‹Ğº Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ°

"""

        mode_prompts = {
            "extract_text": """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ğ’Ğ•Ğ¡Ğ¬ Ñ‚ĞµĞºÑÑ‚ Ñ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹.
Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚:
- Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ ĞºĞ°Ğº # Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº
- Ğ¡Ğ¿Ğ¸ÑĞºĞ¸ ĞºĞ°Ğº - Ğ¿ÑƒĞ½ĞºÑ‚
- Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ² markdown Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ
""",
            "full_analysis": """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹:

## Ğ¢Ğ•ĞšĞ¡Ğ¢ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢Ğ
(Ğ·Ğ´ĞµÑÑŒ Ğ²ĞµÑÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚)

## ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• Ğ˜Ğ”Ğ•Ğ˜
(Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°)

## Ğ¢Ğ•Ğ“Ğ˜
(ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¿ÑÑ‚ÑƒÑ)
""",
            "summary": """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ñ‚ĞµĞºÑÑ‚ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ¹ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸:

## Ğ¢Ğ•ĞšĞ¡Ğ¢
(Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚)

## ĞšĞ ĞĞ¢ĞšĞĞ• Ğ¡ĞĞ”Ğ•Ğ Ğ–ĞĞĞ˜Ğ•
(Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿ÑƒĞ½ĞºÑ‚Ñ‹)
""",
            "key_concepts": """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸:

## Ğ¢Ğ•ĞšĞ¡Ğ¢
(Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚)

## ĞšĞĞĞ¦Ğ•ĞŸĞ¦Ğ˜Ğ˜
| Ğ¢ĞµÑ€Ğ¼Ğ¸Ğ½ | ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ |
"""
        }

        return base + mode_prompts.get(mode, mode_prompts["extract_text"])

    def process_page(self, image: Image.Image, page_num: int, mode: str) -> Dict:
        """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ñƒ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ."""
        system_prompt = self.get_system_prompt(mode)

        try:
            img_base64 = self.image_to_base64(image)

            print(f"   ğŸ”„ ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ğ² {self.config['vision_model']}...")

            response = self.client.generate(
                model=self.config["vision_model"],
                prompt=f"ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ {page_num} Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°:\n\n{system_prompt}",
                images=[img_base64]
            )

            return {
                "page": page_num,
                "mode": mode,
                "processed_at": datetime.now().isoformat(),
                "response": response,
                "status": "success",
                "model": self.config["vision_model"]
            }

        except Exception as e:
            return {
                "page": page_num,
                "mode": mode,
                "processed_at": datetime.now().isoformat(),
                "error": str(e),
                "status": "error"
            }

    def process_pdf(self, pdf_path: str, mode: str = "full_analysis",
                    page_range: Tuple[int, int] = None) -> Dict:
        """ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ PDF Ñ‡ĞµÑ€ĞµĞ· Vision."""
        pdf_path = Path(pdf_path)

        pdf_hash = hashlib.md5(pdf_path.name.encode()).hexdigest()[:8]
        session_id = f"{pdf_path.stem}_local_vision_{pdf_hash}"

        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     ğŸ‘ï¸ Local Vision Processor (Ollama)                       â•‘
â•‘     ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {self.config['vision_model']:<44}â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ PDF
        images = self.pdf_to_images(pdf_path)

        # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½
        if page_range:
            start, end = page_range
            images = images[start-1:end]
            page_start = start
        else:
            page_start = 1

        total = len(images)

        print(f"\nğŸš€ ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ {total} ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ '{mode}'")
        print("=" * 50)

        results = []

        for i, image in enumerate(images):
            page_num = page_start + i
            print(f"\nğŸ“ Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {page_num}/{page_start + total - 1}...")

            result = self.process_page(image, page_num, mode)
            results.append(result)

            if result["status"] == "success":
                print(f"   âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾")
            else:
                print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {result.get('error')}")

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
        final_result = {
            "session_id": session_id,
            "source_pdf": str(pdf_path),
            "mode": mode,
            "processor": "local_ollama_vision",
            "model": self.config["vision_model"],
            "total_pages": total,
            "successful_pages": sum(1 for r in results if r["status"] == "success"),
            "processed_at": datetime.now().isoformat(),
            "results": results
        }

        output_json = self.output_dir / f"{session_id}_processed.json"
        with open(output_json, "w", encoding="utf-8") as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)

        output_md = self.output_dir / f"{session_id}_processed.md"
        self._save_as_markdown(final_result, output_md)

        print("\n" + "=" * 50)
        print(f"âœ… ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°!")
        print(f"ğŸ“Š Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {final_result['successful_pages']}/{total}")
        print(f"ğŸ“ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹: {output_md}")

        return final_result

    def _save_as_markdown(self, result: Dict, output_path: Path):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ² Markdown."""
        md = f"""# ğŸ‘ï¸ Local Vision: {Path(result['source_pdf']).name}

**ĞœĞ¾Ğ´ĞµĞ»ÑŒ:** {result.get('model', 'N/A')}
**Ğ”Ğ°Ñ‚Ğ°:** {result['processed_at']}
**Ğ ĞµĞ¶Ğ¸Ğ¼:** {result['mode']}
**Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†:** {result['total_pages']}

---

"""
        for r in result["results"]:
            if r["status"] == "success":
                md += f"""
## Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {r['page']}

{r['response']}

---
"""

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(md)


class LocalWhisperTranscriber:
    """Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· MLX-Whisper."""

    def __init__(self, model: str = "large-v3"):
        """
        Args:
            model: ĞœĞ¾Ğ´ĞµĞ»ÑŒ whisper (tiny, base, small, medium, large-v3)
        """
        self.model = model
        self.output_dir = OUTPUT_DIR
        self.transcripts_dir = TRANSCRIPTS_DIR

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ mlx_whisper
        try:
            import mlx_whisper
            self.mlx_whisper = mlx_whisper
        except ImportError:
            raise ImportError("Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ: pip install mlx-whisper")

        logger.info(f"LocalWhisperTranscriber Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {model}")

    def transcribe(self, audio_path: str, language: str = "ru") -> Dict:
        """
        Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ» Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· MLX-Whisper.

        Args:
            audio_path: ĞŸÑƒÑ‚ÑŒ Ğº Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ»Ñƒ
            language: Ğ¯Ğ·Ñ‹Ğº (ru, en, etc.)

        Returns:
            Dict Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸
        """
        audio_path = Path(audio_path)
        if not audio_path.exists():
            raise FileNotFoundError(f"Ğ¤Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {audio_path}")

        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     ğŸ™ï¸ Local Whisper Transcriber (MLX)                       â•‘
â•‘     ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {self.model:<46}â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
        print(f"ğŸ“‚ Ğ¤Ğ°Ğ¹Ğ»: {audio_path.name}")
        print(f"ğŸ”„ Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€ÑƒÑ (ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ½ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¸Ğ½ÑƒÑ‚)...")

        # Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· MLX-Whisper
        result = self.mlx_whisper.transcribe(
            str(audio_path),
            path_or_hf_repo=f"mlx-community/whisper-{self.model}-mlx",
            language=language,
            verbose=False
        )

        # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
        text = result.get("text", "")
        segments = []

        for seg in result.get("segments", []):
            segments.append({
                "start": seg.get("start", 0),
                "end": seg.get("end", 0),
                "text": seg.get("text", "")
            })

        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ
        duration = segments[-1]["end"] if segments else 0

        output = {
            "text": text,
            "segments": segments,
            "duration": duration,
            "provider": "mlx_whisper",
            "model": self.model,
            "language": language
        }

        print(f"\nâœ… Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°!")
        print(f"   ğŸ“ {len(text):,} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²")
        print(f"   â±ï¸ {duration / 60:.1f} Ğ¼Ğ¸Ğ½ÑƒÑ‚")

        return output

    def transcribe_and_save(self, audio_path: str, language: str = "ru") -> Tuple[Dict, Path]:
        """Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚."""
        result = self.transcribe(audio_path, language)

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚
        audio_name = Path(audio_path).stem
        transcript_path = self.transcripts_dir / f"{audio_name}_local_transcript.txt"

        with open(transcript_path, "w", encoding="utf-8") as f:
            f.write(result["text"])

        print(f"   ğŸ“ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: {transcript_path}")

        return result, transcript_path


def check_local_requirements() -> Dict[str, bool]:
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²."""
    status = {
        "ollama": False,
        "ollama_models": [],
        "mlx_whisper": False,
        "pdf2image": PDF2IMAGE_AVAILABLE,
        "pdfplumber": PDFPLUMBER_AVAILABLE
    }

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ollama
    try:
        client = OllamaClient()
        status["ollama"] = True
        status["ollama_models"] = client.list_models()
    except:
        pass

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ MLX-Whisper
    try:
        import mlx_whisper
        status["mlx_whisper"] = True
    except:
        pass

    return status


def main():
    """CLI Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ."""
    parser = argparse.ArgumentParser(
        description="Local Processor - Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ollama Ğ¸ MLX-Whisper"
    )

    subparsers = parser.add_subparsers(dest="command", help="ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹")

    # ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° pdf
    pdf_parser = subparsers.add_parser("pdf", help="ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ PDF Ñ‡ĞµÑ€ĞµĞ· Ollama (Ñ‚ĞµĞºÑÑ‚)")
    pdf_parser.add_argument("pdf_path", help="ĞŸÑƒÑ‚ÑŒ Ğº PDF")
    pdf_parser.add_argument("--mode", "-m", default="full_analysis",
                           choices=["summary", "key_concepts", "coaching_tools", "full_analysis"])
    pdf_parser.add_argument("--model", default="llama3.3", help="ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ollama")

    # ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° vision
    vision_parser = subparsers.add_parser("vision", help="ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ PDF Ñ‡ĞµÑ€ĞµĞ· Ollama Vision")
    vision_parser.add_argument("pdf_path", help="ĞŸÑƒÑ‚ÑŒ Ğº PDF")
    vision_parser.add_argument("--mode", "-m", default="full_analysis",
                              choices=["extract_text", "full_analysis"])
    vision_parser.add_argument("--pages", "-p", help="Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† (1-10)")
    vision_parser.add_argument("--model", default="llava:34b", help="Vision Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ")

    # ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° audio
    audio_parser = subparsers.add_parser("audio", help="Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‡ĞµÑ€ĞµĞ· MLX-Whisper")
    audio_parser.add_argument("audio_path", help="ĞŸÑƒÑ‚ÑŒ Ğº Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ»Ñƒ")
    audio_parser.add_argument("--model", default="large-v3",
                             choices=["tiny", "base", "small", "medium", "large-v3"])
    audio_parser.add_argument("--language", "-l", default="ru")

    # ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° check
    check_parser = subparsers.add_parser("check", help="ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    try:
        if args.command == "check":
            status = check_local_requirements()
            print("\nğŸ” Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²:\n")
            print(f"  Ollama: {'âœ…' if status['ollama'] else 'âŒ'}")
            if status['ollama']:
                print(f"    ĞœĞ¾Ğ´ĞµĞ»Ğ¸: {', '.join(status['ollama_models'][:5])}...")
            print(f"  MLX-Whisper: {'âœ…' if status['mlx_whisper'] else 'âŒ'}")
            print(f"  pdf2image: {'âœ…' if status['pdf2image'] else 'âŒ'}")
            print(f"  pdfplumber: {'âœ…' if status['pdfplumber'] else 'âŒ'}")

        elif args.command == "pdf":
            processor = LocalPDFProcessor({"text_model": args.model})
            processor.process_pdf(args.pdf_path, mode=args.mode)

        elif args.command == "vision":
            page_range = None
            if args.pages:
                parts = args.pages.split("-")
                page_range = (int(parts[0]), int(parts[1]))

            processor = LocalVisionProcessor({"vision_model": args.model})
            processor.process_pdf(args.pdf_path, mode=args.mode, page_range=page_range)

        elif args.command == "audio":
            transcriber = LocalWhisperTranscriber(model=args.model)
            transcriber.transcribe_and_save(args.audio_path, language=args.language)

    except Exception as e:
        print(f"\nâŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
