#!/usr/bin/env python3
"""
Audio Transcription & Processing –¥–ª—è SKOLKOVO Executive Coaching
=================================================================
–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π —Å–µ–º–∏–Ω–∞—Ä–æ–≤ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Claude API.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã:
- OpenAI Whisper API (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞)
- –õ–æ–∫–∞–ª—å–Ω—ã–π Whisper (–±–µ—Å–ø–ª–∞—Ç–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏)
- AssemblyAI (—Ö–æ—Ä–æ—à –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π)

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ –ª—é–±–æ–π –¥–ª–∏–Ω—ã
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
- –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Claude API
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π PDF
"""

import os
import sys
import json
import math
import tempfile
import subprocess
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
import argparse

# Audio processing
try:
    from pydub import AudioSegment
    PYDUB_AVAILABLE = True
except ImportError:
    PYDUB_AVAILABLE = False
    print("‚ö†Ô∏è pydub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pydub")

# API clients
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import assemblyai as aai
    ASSEMBLYAI_AVAILABLE = True
except ImportError:
    ASSEMBLYAI_AVAILABLE = False


# Configuration
DEFAULT_CONFIG = {
    "transcription_provider": "openai",  # openai, local_whisper, assemblyai
    "whisper_model": "whisper-1",  # –î–ª—è OpenAI API
    "local_whisper_model": "large-v3",  # –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ Whisper
    "language": "ru",
    "max_chunk_duration_minutes": 20,  # Whisper API –ª–∏–º–∏—Ç ~25MB
    "claude_model": "claude-sonnet-4-20250514",
    "max_output_tokens": 4096
}


class AudioTranscriber:
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏."""
    
    def __init__(self, provider: str = "openai", api_key: str = None):
        """
        Args:
            provider: openai, local_whisper, assemblyai
            api_key: API –∫–ª—é—á –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        """
        self.provider = provider
        self.api_key = api_key
        
        if provider == "openai":
            if not OPENAI_AVAILABLE:
                raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ openai: pip install openai")
            self.client = openai.OpenAI(api_key=api_key or os.environ.get("OPENAI_API_KEY"))
        
        elif provider == "assemblyai":
            if not ASSEMBLYAI_AVAILABLE:
                raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ assemblyai: pip install assemblyai")
            aai.settings.api_key = api_key or os.environ.get("ASSEMBLYAI_API_KEY")
        
        elif provider == "local_whisper":
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ whisper
            try:
                result = subprocess.run(["whisper", "--help"], capture_output=True)
            except FileNotFoundError:
                raise ImportError("–õ–æ–∫–∞–ª—å–Ω—ã–π Whisper –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai-whisper")
    
    def transcribe(self, audio_path: str, language: str = "ru") -> Dict:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª.
        
        Returns:
            Dict: {
                "text": str,
                "segments": List[Dict],  # –° —Ç–∞–π–º–∫–æ–¥–∞–º–∏
                "duration": float,
                "provider": str
            }
        """
        audio_path = Path(audio_path)
        if not audio_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}")
        
        print(f"üéôÔ∏è –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é: {audio_path.name}")
        print(f"   –ü—Ä–æ–≤–∞–π–¥–µ—Ä: {self.provider}")
        
        if self.provider == "openai":
            return self._transcribe_openai(audio_path, language)
        elif self.provider == "assemblyai":
            return self._transcribe_assemblyai(audio_path, language)
        elif self.provider == "local_whisper":
            return self._transcribe_local(audio_path, language)
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.provider}")
    
    def _transcribe_openai(self, audio_path: Path, language: str) -> Dict:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI Whisper API."""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (–ª–∏–º–∏—Ç 25MB)
        file_size_mb = audio_path.stat().st_size / (1024 * 1024)
        
        if file_size_mb > 24:
            print(f"   ‚ö†Ô∏è –§–∞–π–ª {file_size_mb:.1f}MB > 24MB, —Ä–∞–∑–±–∏–≤–∞—é –Ω–∞ —á–∞—Å—Ç–∏...")
            return self._transcribe_openai_chunked(audio_path, language)
        
        with open(audio_path, "rb") as audio_file:
            response = self.client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language=language,
                response_format="verbose_json",
                timestamp_granularities=["segment"]
            )
        
        return {
            "text": response.text,
            "segments": [
                {
                    "start": s.start,
                    "end": s.end,
                    "text": s.text
                }
                for s in (response.segments or [])
            ],
            "duration": response.duration,
            "provider": "openai"
        }
    
    def _transcribe_openai_chunked(self, audio_path: Path, language: str) -> Dict:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –ø–æ —á–∞—Å—Ç—è–º."""
        
        if not PYDUB_AVAILABLE:
            raise ImportError("–î–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ —Ñ–∞–π–ª–æ–≤ –Ω—É–∂–µ–Ω pydub: pip install pydub")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        print("   üìÇ –ó–∞–≥—Ä—É–∂–∞—é –∞—É–¥–∏–æ...")
        audio = AudioSegment.from_file(str(audio_path))
        
        total_duration = len(audio) / 1000  # –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        chunk_duration = DEFAULT_CONFIG["max_chunk_duration_minutes"] * 60 * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
        
        chunks = []
        for i in range(0, len(audio), chunk_duration):
            chunks.append(audio[i:i + chunk_duration])
        
        print(f"   üì¶ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π")
        
        all_text = []
        all_segments = []
        time_offset = 0
        
        for i, chunk in enumerate(chunks, 1):
            print(f"   üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —á–∞—Å—Ç—å {i}/{len(chunks)}...")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as tmp:
                chunk.export(tmp.name, format="mp3")
                tmp_path = tmp.name
            
            try:
                with open(tmp_path, "rb") as audio_file:
                    response = self.client.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_file,
                        language=language,
                        response_format="verbose_json",
                        timestamp_granularities=["segment"]
                    )
                
                all_text.append(response.text)
                
                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Ç–∞–π–º–∫–æ–¥—ã
                for s in (response.segments or []):
                    all_segments.append({
                        "start": s.start + time_offset,
                        "end": s.end + time_offset,
                        "text": s.text
                    })
                
                time_offset += len(chunk) / 1000
                
            finally:
                os.unlink(tmp_path)
        
        return {
            "text": " ".join(all_text),
            "segments": all_segments,
            "duration": total_duration,
            "provider": "openai"
        }
    
    def _transcribe_assemblyai(self, audio_path: Path, language: str) -> Dict:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —á–µ—Ä–µ–∑ AssemblyAI."""
        
        config = aai.TranscriptionConfig(
            language_code=language,
            punctuate=True,
            format_text=True
        )
        
        transcriber = aai.Transcriber()
        transcript = transcriber.transcribe(str(audio_path), config=config)
        
        if transcript.status == aai.TranscriptStatus.error:
            raise Exception(f"AssemblyAI –æ—à–∏–±–∫–∞: {transcript.error}")
        
        segments = []
        if transcript.words:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –≤ —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ ~30 —Å–µ–∫—É–Ω–¥
            current_segment = {"start": 0, "end": 0, "text": ""}
            for word in transcript.words:
                if word.start / 1000 - current_segment["start"] > 30:
                    if current_segment["text"]:
                        segments.append(current_segment)
                    current_segment = {
                        "start": word.start / 1000,
                        "end": word.end / 1000,
                        "text": word.text
                    }
                else:
                    current_segment["end"] = word.end / 1000
                    current_segment["text"] += " " + word.text
            
            if current_segment["text"]:
                segments.append(current_segment)
        
        return {
            "text": transcript.text,
            "segments": segments,
            "duration": transcript.audio_duration,
            "provider": "assemblyai"
        }
    
    def _transcribe_local(self, audio_path: Path, language: str) -> Dict:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ã–π Whisper."""
        
        model = DEFAULT_CONFIG["local_whisper_model"]
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≤—ã–≤–æ–¥–∞
        with tempfile.TemporaryDirectory() as tmp_dir:
            cmd = [
                "whisper",
                str(audio_path),
                "--model", model,
                "--language", language,
                "--output_format", "json",
                "--output_dir", tmp_dir
            ]
            
            print(f"   üîÑ –ó–∞–ø—É—Å–∫–∞—é –ª–æ–∫–∞–ª—å–Ω—ã–π Whisper (–º–æ–¥–µ–ª—å: {model})...")
            subprocess.run(cmd, check=True)
            
            # –ß–∏—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            json_file = Path(tmp_dir) / f"{audio_path.stem}.json"
            with open(json_file, encoding="utf-8") as f:
                result = json.load(f)
        
        return {
            "text": result.get("text", ""),
            "segments": [
                {
                    "start": s["start"],
                    "end": s["end"],
                    "text": s["text"]
                }
                for s in result.get("segments", [])
            ],
            "duration": result.get("segments", [{}])[-1].get("end", 0) if result.get("segments") else 0,
            "provider": "local_whisper"
        }


class SeminarProcessor:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π —Å–µ–º–∏–Ω–∞—Ä–æ–≤ —á–µ—Ä–µ–∑ Claude API."""
    
    def __init__(self, anthropic_api_key: str = None):
        self.api_key = anthropic_api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ANTHROPIC_API_KEY")
        
        self.client = anthropic.Anthropic(api_key=self.api_key)
        
        self.base_dir = Path(__file__).parent
        self.output_dir = self.base_dir / "output"
        self.transcripts_dir = self.base_dir / "transcripts"
        self.knowledge_dir = self.base_dir / "knowledge_base"
        
        for d in [self.output_dir, self.transcripts_dir, self.knowledge_dir]:
            d.mkdir(exist_ok=True)
    
    def process_seminar(self, transcript: Dict, metadata: Dict = None,
                        mode: str = "full_analysis") -> Dict:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Å–µ–º–∏–Ω–∞—Ä–∞.
        
        Args:
            transcript: –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            metadata: –î–æ–ø. –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–Ω–∞–∑–≤–∞–Ω–∏–µ, –¥–∞—Ç–∞, —Å–ø–∏–∫–µ—Ä)
            mode: –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        metadata = metadata or {}
        
        text = transcript["text"]
        duration_min = transcript.get("duration", 0) / 60
        
        print(f"\nüìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å–µ–º–∏–Ω–∞—Ä ({duration_min:.0f} –º–∏–Ω)...")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ ~15 –º–∏–Ω—É—Ç —Ç–µ–∫—Å—Ç–∞
        chunks = self._chunk_by_time(transcript)
        
        print(f"   üì¶ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π")
        
        results = []
        
        for i, chunk in enumerate(chunks, 1):
            print(f"   üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —á–∞—Å—Ç—å {i}/{len(chunks)}...")
            
            result = self._process_chunk(chunk, metadata, mode, i, len(chunks))
            results.append(result)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏
        print("   üìä –°–æ–∑–¥–∞—é –æ–±—â–µ–µ —Å–∞–º–º–∞—Ä–∏...")
        final_summary = self._create_final_summary(results, metadata)
        
        return {
            "metadata": metadata,
            "transcript": transcript,
            "chunk_results": results,
            "final_summary": final_summary,
            "processed_at": datetime.now().isoformat()
        }
    
    def _chunk_by_time(self, transcript: Dict, chunk_minutes: int = 15) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏."""
        
        segments = transcript.get("segments", [])
        
        if not segments:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–æ–≤, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å–ª–æ–≤–∞–º
            text = transcript["text"]
            words = text.split()
            words_per_chunk = len(words) // max(1, int(transcript.get("duration", 600) / 60 / chunk_minutes))
            words_per_chunk = max(500, words_per_chunk)
            
            chunks = []
            for i in range(0, len(words), words_per_chunk):
                chunk_text = " ".join(words[i:i + words_per_chunk])
                chunks.append({
                    "text": chunk_text,
                    "start_time": "N/A",
                    "end_time": "N/A"
                })
            return chunks
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        chunk_seconds = chunk_minutes * 60
        chunks = []
        current_chunk = {"text": "", "start_time": 0, "end_time": 0, "segments": []}
        
        for segment in segments:
            if segment["start"] - current_chunk["start_time"] > chunk_seconds and current_chunk["text"]:
                current_chunk["end_time"] = current_chunk["segments"][-1]["end"]
                chunks.append(current_chunk)
                current_chunk = {"text": "", "start_time": segment["start"], "end_time": 0, "segments": []}
            
            current_chunk["text"] += " " + segment["text"]
            current_chunk["segments"].append(segment)
        
        if current_chunk["text"]:
            current_chunk["end_time"] = current_chunk["segments"][-1]["end"] if current_chunk["segments"] else 0
            chunks.append(current_chunk)
        
        return chunks
    
    def _format_time(self, seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –≤ HH:MM:SS."""
        return str(timedelta(seconds=int(seconds)))
    
    def _process_chunk(self, chunk: Dict, metadata: Dict, mode: str,
                       chunk_num: int, total_chunks: int) -> Dict:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —á–∞–Ω–∫ —á–µ—Ä–µ–∑ Claude."""
        
        system_prompt = """–¢—ã - AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–æ–≥—Ä–∞–º–º—ã Executive Coaching & Mentoring –≤ –°–ö–û–õ–ö–û–í–û.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Å–µ–º–∏–Ω–∞—Ä–∞ –∏ –∏–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

–ö–û–ù–¢–ï–ö–°–¢:
- –≠—Ç–æ –∞—É–¥–∏–æ–∑–∞–ø–∏—Å—å —Å–µ–º–∏–Ω–∞—Ä–∞ –ø–æ –∫–æ—É—á–∏–Ω–≥—É
- –£—Å—Ç–Ω–∞—è —Ä–µ—á—å: –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–≤—Ç–æ—Ä—ã, –æ–≥–æ–≤–æ—Ä–∫–∏, –Ω–µ–ø–æ–ª–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- –í–∞–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å —Å—É—Ç—å, –∞ –Ω–µ –¥–æ—Å–ª–æ–≤–Ω—ã–π —Ç–µ–∫—Å—Ç

–ü–†–ò–ù–¶–ò–ü–´ –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø:
1. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö –∏–¥–µ—è—Ö –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è—Ö
2. –í—ã–¥–µ–ª—è–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–∏–∫–∏
3. –û—Ç–º–µ—á–∞–π –ø—Ä–∏–º–µ—Ä—ã –∏ –∫–µ–π—Å—ã
4. –§–∏–∫—Å–∏—Ä—É–π –º–æ—â–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å–ø–∏–∫–µ—Ä–∞
5. –ò–≥–Ω–æ—Ä–∏—Ä—É–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—É–∑—ã, –ø–æ–≤—Ç–æ—Ä—ã, –º–µ–∂–¥–æ–º–µ—Ç–∏—è

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:

## –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã
[3-5 –≥–ª–∞–≤–Ω—ã—Ö —Ç–µ–º —ç—Ç–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞]

## –û—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏
[–ö—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º—ã—Å–ª–µ–π]

## –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–∏–∫–∏
[–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã, —É–ø–æ–º—è–Ω—É—Ç—ã–µ —Å–ø–∏–∫–µ—Ä–æ–º]

## –ü—Ä–∏–º–µ—Ä—ã –∏ –∫–µ–π—Å—ã
[–ò—Å—Ç–æ—Ä–∏–∏, –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –ø—Ä–∞–∫—Ç–∏–∫–∏]

## –ú–æ—â–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
[–í–æ–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–¥–∞–≤–∞–ª —Å–ø–∏–∫–µ—Ä]

## –¶–∏—Ç–∞—Ç—ã
[–Ø—Ä–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–æ–∏—Ç –∑–∞–ø–æ–º–Ω–∏—Ç—å]

## –°–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏
[–ö–∞–∫ —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ–º–∞–º–∏ –∫–æ—É—á–∏–Ω–≥–∞]
"""
        
        start_time = self._format_time(chunk.get("start_time", 0)) if isinstance(chunk.get("start_time"), (int, float)) else chunk.get("start_time", "N/A")
        end_time = self._format_time(chunk.get("end_time", 0)) if isinstance(chunk.get("end_time"), (int, float)) else chunk.get("end_time", "N/A")
        
        user_message = f"""–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –°–ï–ú–ò–ù–ê–†–ï:
- –ù–∞–∑–≤–∞–Ω–∏–µ: {metadata.get('title', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
- –°–ø–∏–∫–µ—Ä: {metadata.get('speaker', '–ù–µ —É–∫–∞–∑–∞–Ω')}
- –î–∞—Ç–∞: {metadata.get('date', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}
- –ú–æ–¥—É–ª—å: {metadata.get('module', '–ù–µ —É–∫–∞–∑–∞–Ω')}

–§–†–ê–ì–ú–ï–ù–¢ {chunk_num}/{total_chunks}
–í—Ä–µ–º—è: {start_time} - {end_time}

---
–¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø:

{chunk['text']}

---

–û–±—Ä–∞–±–æ—Ç–∞–π —ç—Ç–æ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Å–µ–º–∏–Ω–∞—Ä–∞ –∏ –∏–∑–≤–ª–µ–∫–∏ –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."""

        response = self.client.messages.create(
            model=DEFAULT_CONFIG["claude_model"],
            max_tokens=DEFAULT_CONFIG["max_output_tokens"],
            system=system_prompt,
            messages=[{"role": "user", "content": user_message}]
        )
        
        return {
            "chunk_num": chunk_num,
            "time_range": f"{start_time} - {end_time}",
            "response": response.content[0].text,
            "usage": {
                "input_tokens": response.usage.input_tokens,
                "output_tokens": response.usage.output_tokens
            }
        }
    
    def _create_final_summary(self, chunk_results: List[Dict], metadata: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–µ —Å–∞–º–º–∞—Ä–∏ –∏–∑ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤."""
        
        all_content = "\n\n---\n\n".join([
            f"–ß–ê–°–¢–¨ {r['chunk_num']} ({r['time_range']}):\n{r['response']}"
            for r in chunk_results
        ])
        
        system_prompt = """–¢—ã - AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–æ–≥—Ä–∞–º–º—ã Executive Coaching & Mentoring –≤ –°–ö–û–õ–ö–û–í–û.
–°–æ–∑–¥–∞–π –∏—Ç–æ–≥–æ–≤–æ–µ —Å–∞–º–º–∞—Ä–∏ —Å–µ–º–∏–Ω–∞—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.

–ó–ê–î–ê–ß–ê:
1. –û–±—ä–µ–¥–∏–Ω–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π
2. –£–±–µ—Ä–∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
3. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
4. –í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—ã–µ takeaways

–§–û–†–ú–ê–¢:

# üìö –°–∞–º–º–∞—Ä–∏ —Å–µ–º–∏–Ω–∞—Ä–∞

## üéØ –ì–ª–∞–≤–Ω—ã–µ –∏–¥–µ–∏ (3-5 –ø—É–Ω–∫—Ç–æ–≤)
[–°–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –∏–∑ —Å–µ–º–∏–Ω–∞—Ä–∞]

## üîß –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–∏–∫–∏
[–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã]

## ‚ùì –ú–æ—â–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
[–í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –∏ –ø—Ä–∞–∫—Ç–∏–∫–∏]

## üí° –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã
[–ù–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–æ–∏—Ç –∑–∞–ø–æ–º–Ω–∏—Ç—å]

## üîó –°–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ–º–∞–º–∏
[–ö–∞–∫ —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å –ø—Ä–æ–≥—Ä–∞–º–º–æ–π]

## ‚úÖ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —à–∞–≥–∏
[–ß—Ç–æ –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Å—Ä–∞–∑—É]

## üè∑Ô∏è –¢–µ–≥–∏
[5-10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞]
"""
        
        user_message = f"""–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –°–ï–ú–ò–ù–ê–†–ï:
- –ù–∞–∑–≤–∞–Ω–∏–µ: {metadata.get('title', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
- –°–ø–∏–∫–µ—Ä: {metadata.get('speaker', '–ù–µ —É–∫–∞–∑–∞–Ω')}
- –î–∞—Ç–∞: {metadata.get('date', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}
- –ú–æ–¥—É–ª—å: {metadata.get('module', '–ù–µ —É–∫–∞–∑–∞–Ω')}

–û–ë–†–ê–ë–û–¢–ê–ù–ù–´–ï –§–†–ê–ì–ú–ï–ù–¢–´:

{all_content}

---

–°–æ–∑–¥–∞–π –∏—Ç–æ–≥–æ–≤–æ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ –≤—Å–µ–≥–æ —Å–µ–º–∏–Ω–∞—Ä–∞."""

        response = self.client.messages.create(
            model=DEFAULT_CONFIG["claude_model"],
            max_tokens=DEFAULT_CONFIG["max_output_tokens"],
            system=system_prompt,
            messages=[{"role": "user", "content": user_message}]
        )
        
        return response.content[0].text
    
    def save_results(self, results: Dict, output_name: str = None) -> Tuple[Path, Path]:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        
        if output_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            title = results.get("metadata", {}).get("title", "seminar")
            title_clean = "".join(c for c in title if c.isalnum() or c in " _-")[:30]
            output_name = f"{title_clean}_{timestamp}"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        json_path = self.output_dir / f"{output_name}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º Markdown
        md_path = self.output_dir / f"{output_name}.md"
        
        md_content = f"""# {results.get('metadata', {}).get('title', '–°–µ–º–∏–Ω–∞—Ä')}

**–°–ø–∏–∫–µ—Ä:** {results.get('metadata', {}).get('speaker', '–ù–µ —É–∫–∞–∑–∞–Ω')}  
**–î–∞—Ç–∞:** {results.get('metadata', {}).get('date', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}  
**–ú–æ–¥—É–ª—å:** {results.get('metadata', {}).get('module', '–ù–µ —É–∫–∞–∑–∞–Ω')}  
**–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {results.get('transcript', {}).get('duration', 0) / 60:.0f} –º–∏–Ω—É—Ç  
**–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ:** {results.get('processed_at', '')}

---

{results.get('final_summary', '')}

---

## –î–µ—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ —á–∞—Å—Ç—è–º

"""
        
        for chunk in results.get("chunk_results", []):
            md_content += f"""
### –ß–∞—Å—Ç—å {chunk['chunk_num']} ({chunk['time_range']})

{chunk['response']}

---
"""
        
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(md_content)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∏—Å—Ç—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        transcript_path = self.transcripts_dir / f"{output_name}_transcript.txt"
        with open(transcript_path, "w", encoding="utf-8") as f:
            f.write(results.get("transcript", {}).get("text", ""))
        
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   üìÑ JSON: {json_path}")
        print(f"   üìù Markdown: {md_path}")
        print(f"   üéôÔ∏è –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç: {transcript_path}")
        
        return json_path, md_path


def process_audio_file(audio_path: str, 
                       transcription_provider: str = "openai",
                       transcription_api_key: str = None,
                       anthropic_api_key: str = None,
                       metadata: Dict = None,
                       mode: str = "full_analysis") -> Dict:
    """
    –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –æ–±—Ä–∞–±–æ—Ç–∫–∞.
    
    Args:
        audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ —Ñ–∞–π–ª—É
        transcription_provider: openai, local_whisper, assemblyai
        transcription_api_key: API –∫–ª—é—á –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        anthropic_api_key: API –∫–ª—é—á –¥–ª—è Claude
        metadata: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ–º–∏–Ω–∞—Ä–µ
        mode: –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
    transcriber = AudioTranscriber(
        provider=transcription_provider,
        api_key=transcription_api_key
    )
    
    transcript = transcriber.transcribe(audio_path)
    
    print(f"\n‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
    print(f"   üìù {len(transcript['text'])} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   ‚è±Ô∏è {transcript['duration'] / 60:.1f} –º–∏–Ω—É—Ç")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Claude
    processor = SeminarProcessor(anthropic_api_key=anthropic_api_key)
    
    results = processor.process_seminar(transcript, metadata, mode)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    processor.save_results(results)
    
    return results


def main():
    parser = argparse.ArgumentParser(
        description="–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–µ–π —Å–µ–º–∏–Ω–∞—Ä–æ–≤"
    )
    
    parser.add_argument("audio_path", help="–ü—É—Ç—å –∫ –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ —Ñ–∞–π–ª—É")
    
    parser.add_argument(
        "--provider", "-p",
        choices=["openai", "local_whisper", "assemblyai"],
        default="openai",
        help="–ü—Ä–æ–≤–∞–π–¥–µ—Ä —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (default: openai)"
    )
    
    parser.add_argument("--title", "-t", help="–ù–∞–∑–≤–∞–Ω–∏–µ —Å–µ–º–∏–Ω–∞—Ä–∞")
    parser.add_argument("--speaker", "-s", help="–°–ø–∏–∫–µ—Ä")
    parser.add_argument("--date", "-d", help="–î–∞—Ç–∞ —Å–µ–º–∏–Ω–∞—Ä–∞")
    parser.add_argument("--module", "-m", help="–ù–æ–º–µ—Ä –º–æ–¥—É–ª—è")
    
    parser.add_argument("--openai-key", help="OpenAI API –∫–ª—é—á")
    parser.add_argument("--anthropic-key", help="Anthropic API –∫–ª—é—á")
    parser.add_argument("--assemblyai-key", help="AssemblyAI API –∫–ª—é—á")
    
    parser.add_argument(
        "--transcript-only",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏ Claude"
    )
    
    args = parser.parse_args()
    
    # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {}
    if args.title:
        metadata["title"] = args.title
    if args.speaker:
        metadata["speaker"] = args.speaker
    if args.date:
        metadata["date"] = args.date
    if args.module:
        metadata["module"] = args.module
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º API –∫–ª—é—á –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    if args.provider == "openai":
        transcription_key = args.openai_key
    elif args.provider == "assemblyai":
        transcription_key = args.assemblyai_key
    else:
        transcription_key = None
    
    try:
        if args.transcript_only:
            # –¢–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            transcriber = AudioTranscriber(
                provider=args.provider,
                api_key=transcription_key
            )
            transcript = transcriber.transcribe(args.audio_path)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
            output_path = Path(args.audio_path).with_suffix(".txt")
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(transcript["text"])
            
            print(f"\n‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
            
        else:
            # –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            process_audio_file(
                args.audio_path,
                transcription_provider=args.provider,
                transcription_api_key=transcription_key,
                anthropic_api_key=args.anthropic_key,
                metadata=metadata
            )
    
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
