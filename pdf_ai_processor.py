#!/usr/bin/env python3
"""
PDF AI Processor –¥–ª—è SKOLKOVO Executive Coaching Program
=========================================================
–°–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö PDF —á–µ—Ä–µ–∑ Claude API —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º.

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF (–≤–∫–ª—é—á–∞—è OCR –¥–ª—è —Å–∫–∞–Ω–æ–≤)
- –£–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ —Å —É—á–µ—Ç–æ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü
- –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Claude API —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
- –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º
"""

import os
import sys
import json
import hashlib
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import argparse

# PDF processing
try:
    import pdfplumber
    from pypdf import PdfReader
except ImportError:
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install pdfplumber pypdf")
    sys.exit(1)

# Claude API
try:
    import anthropic
except ImportError:
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ anthropic: pip install anthropic")
    sys.exit(1)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É—Ç–∏–ª–∏—Ç—ã
sys.path.insert(0, str(Path(__file__).parent))
from utils import (
    get_logger, get_rate_limiter, load_config,
    OUTPUT_DIR, CHUNKS_DIR, KNOWLEDGE_DIR, load_env_file
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
load_env_file()
logger = get_logger(__name__)
rate_limiter = get_rate_limiter()

# Configuration
DEFAULT_CONFIG = {
    "model": "claude-sonnet-4-20250514",
    "max_tokens_per_chunk": 4000,  # ~16K —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞
    "max_output_tokens": 4096,
    "overlap_tokens": 200,  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    "language": "ru",
    "processing_modes": {
        "summary": "–°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ —ç—Ç–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞",
        "key_concepts": "–ò–∑–≤–ª–µ–∫–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, –º–æ–¥–µ–ª–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã",
        "coaching_tools": "–ò–∑–≤–ª–µ–∫–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è –∫–æ—É—á–∏–Ω–≥–∞",
        "questions": "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª—É",
        "full_analysis": "–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑: —Å–∞–º–º–∞—Ä–∏ + –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ + –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã + –≤–æ–ø—Ä–æ—Å—ã"
    }
}

class PDFAIProcessor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF —á–µ—Ä–µ–∑ Claude API."""

    def __init__(self, api_key: str = None, config: dict = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–µ—Ä–µ–¥–∞–π—Ç–µ –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ANTHROPIC_API_KEY")

        self.client = anthropic.Anthropic(api_key=self.api_key)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∏–∑ utils, –º–µ—Ä–¥–∂–∏–º —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º
        base_config = load_config()
        self.config = {**DEFAULT_CONFIG, **base_config, **(config or {})}

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç–∏ –∏–∑ utils
        self.output_dir = OUTPUT_DIR
        self.chunks_dir = CHUNKS_DIR
        self.knowledge_dir = KNOWLEDGE_DIR

        logger.info(f"PDFAIProcessor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –º–æ–¥–µ–ª—å: {self.config['model']}")
    
    def extract_text_from_pdf(self, pdf_path: str) -> Tuple[str, Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
        
        Returns:
            Tuple[str, Dict]: (—Ç–µ–∫—Å—Ç, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")
        
        metadata = {
            "filename": pdf_path.name,
            "path": str(pdf_path),
            "extracted_at": datetime.now().isoformat(),
            "pages": 0,
            "characters": 0
        }
        
        full_text = []
        
        print(f"üìÑ –ò–∑–≤–ª–µ–∫–∞—é —Ç–µ–∫—Å—Ç –∏–∑: {pdf_path.name}")
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                metadata["pages"] = len(pdf.pages)
                
                for i, page in enumerate(pdf.pages, 1):
                    print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {i}/{metadata['pages']}...", end="\r")
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                    text = page.extract_text() or ""
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
                    tables = page.extract_tables()
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_content = f"\n\n--- –°–¢–†–ê–ù–ò–¶–ê {i} ---\n\n{text}"
                    
                    if tables:
                        page_content += "\n\n[–¢–ê–ë–õ–ò–¶–´ –ù–ê –°–¢–†–ê–ù–ò–¶–ï]\n"
                        for j, table in enumerate(tables, 1):
                            page_content += f"\n–¢–∞–±–ª–∏—Ü–∞ {j}:\n"
                            for row in table:
                                if row:
                                    page_content += " | ".join(str(cell) if cell else "" for cell in row) + "\n"
                    
                    full_text.append(page_content)
        
        except Exception as e:
            print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF: {e}")
            # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥
            try:
                reader = PdfReader(pdf_path)
                metadata["pages"] = len(reader.pages)
                for i, page in enumerate(reader.pages, 1):
                    text = page.extract_text() or ""
                    full_text.append(f"\n\n--- –°–¢–†–ê–ù–ò–¶–ê {i} ---\n\n{text}")
            except Exception as e2:
                raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç: {e2}")
        
        result_text = "\n".join(full_text)
        metadata["characters"] = len(result_text)
        
        print(f"\n‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {metadata['pages']} —Å—Ç—Ä–∞–Ω–∏—Ü, {metadata['characters']:,} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return result_text, metadata
    
    def smart_chunk_text(self, text: str, max_chars: int = None) -> List[Dict]:
        """
        –£–º–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü.
        
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if max_chars is None:
            max_chars = self.config["max_tokens_per_chunk"] * 4  # ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        
        overlap_chars = self.config["overlap_tokens"] * 4
        
        chunks = []
        
        # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        separators = [
            r'\n\n--- –°–¢–†–ê–ù–ò–¶–ê \d+ ---\n\n',  # –ì—Ä–∞–Ω–∏—Ü—ã —Å—Ç—Ä–∞–Ω–∏—Ü
            r'\n\n#{1,3} ',  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ markdown
            r'\n\n[–ê-–ØA-Z][–ê-–Ø–∞-—èA-Za-z\s]+:\n',  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å –¥–≤–æ–µ—Ç–æ—á–∏–µ–º
            r'\n\n\d+\.\s',  # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
            r'\n\n',  # –î–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å
            r'\n',  # –û–¥–∏–Ω–∞—Ä–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å
            r'\. ',  # –¢–æ—á–∫–∞ —Å –ø—Ä–æ–±–µ–ª–æ–º
        ]
        
        current_pos = 0
        chunk_num = 0
        
        while current_pos < len(text):
            chunk_end = min(current_pos + max_chars, len(text))
            
            # –ï—Å–ª–∏ –Ω–µ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞, –∏—â–µ–º —Ö–æ—Ä–æ—à—É—é —Ç–æ—á–∫—É —Ä–∞–∑—Ä—ã–≤–∞
            if chunk_end < len(text):
                best_break = None
                
                # –ò—â–µ–º —Ä–∞–∑—Ä—ã–≤ –Ω–∞—á–∏–Ω–∞—è —Å –∫–æ–Ω—Ü–∞ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
                search_start = max(current_pos + max_chars // 2, current_pos)
                search_text = text[search_start:chunk_end]
                
                for sep in separators:
                    matches = list(re.finditer(sep, search_text))
                    if matches:
                        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –¥–æ–ø—É—Å—Ç–∏–º–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
                        best_break = search_start + matches[-1].end()
                        break
                
                if best_break:
                    chunk_end = best_break
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
            chunk_text = text[current_pos:chunk_end].strip()
            
            if chunk_text:
                chunk_num += 1
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ —á–∞–Ω–∫–µ
                page_matches = re.findall(r'--- –°–¢–†–ê–ù–ò–¶–ê (\d+) ---', chunk_text)
                pages = [int(p) for p in page_matches] if page_matches else []
                
                chunks.append({
                    "chunk_id": chunk_num,
                    "text": chunk_text,
                    "char_start": current_pos,
                    "char_end": chunk_end,
                    "char_count": len(chunk_text),
                    "pages": pages,
                    "page_range": f"{min(pages)}-{max(pages)}" if pages else "N/A"
                })
            
            # –î–≤–∏–≥–∞–µ–º—Å—è —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            current_pos = chunk_end - overlap_chars if chunk_end < len(text) else chunk_end
        
        print(f"üì¶ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        return chunks
    
    def get_system_prompt(self, mode: str, context: str = "") -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        
        base_prompt = """–¢—ã - AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º—ã Executive Coaching & Mentoring –≤ –°–ö–û–õ–ö–û–í–û.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —É—á–µ–±–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –∏ –∏–∑–≤–ª–µ—á—å –∏–∑ –Ω–µ–≥–æ —Ü–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

–ö–û–ù–¢–ï–ö–°–¢ –ü–†–û–ì–†–ê–ú–ú–´:
- –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é –∫–æ—É—á–∏–Ω–≥–æ–≤—ã—Ö –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –¥–ª—è —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π
- –§–æ–∫—É—Å –Ω–∞ executive-–∫–æ—É—á–∏–Ω–≥ (–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –∏ –≥—Ä—É–ø–ø–æ–≤–æ–π)
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –≤ –∫–æ—É—á–∏–Ω–≥
- –†–∞–∑–≤–∏—Ç–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –∏ –ª–∏–¥–µ—Ä—Å—Ç–≤–∞

–ü–†–ò–ù–¶–ò–ü–´ –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø:
1. –°–æ—Ö—Ä–∞–Ω—è–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
2. –í—ã–¥–µ–ª—è–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–∏–∫–∏
3. –û—Ç–º–µ—á–∞–π —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏
4. –£–∫–∞–∑—ã–≤–∞–π –∞–≤—Ç–æ—Ä–æ–≤ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –µ—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è
5. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ª–µ–≥–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞

"""
        
        mode_prompts = {
            "summary": """–ó–ê–î–ê–ß–ê: –°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
## –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã
[–ü–µ—Ä–µ—á–∏—Å–ª–∏ 3-5 –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º]

## –ö–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏
[–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≥–ª–∞–≤–Ω—ã—Ö –∏–¥–µ–π]

## –í–∞–∂–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã/–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
[–¢–æ—á–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–æ–∏—Ç –∑–∞–ø–æ–º–Ω–∏—Ç—å]

## –°–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏
[–ö–∞–∫ —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å –∫–æ—É—á–∏–Ω–≥–æ–º/–ª–∏–¥–µ—Ä—Å—Ç–≤–æ–º]
""",
            
            "key_concepts": """–ó–ê–î–ê–ß–ê: –ò–∑–≤–ª–µ–∫–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, –º–æ–¥–µ–ª–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{
    "concepts": [
        {
            "name": "–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏",
            "definition": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ",
            "author": "–ê–≤—Ç–æ—Ä (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–µ–Ω)",
            "application": "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –∫–æ—É—á–∏–Ω–≥–µ"
        }
    ],
    "models": [
        {
            "name": "–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏",
            "components": ["–∫–æ–º–ø–æ–Ω–µ–Ω—Ç1", "–∫–æ–º–ø–æ–Ω–µ–Ω—Ç2"],
            "usage": "–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å"
        }
    ],
    "tools": [
        {
            "name": "–ù–∞–∑–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞",
            "description": "–û–ø–∏—Å–∞–Ω–∏–µ",
            "when_to_use": "–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å"
        }
    ]
}
""",
            
            "coaching_tools": """–ó–ê–î–ê–ß–ê: –ò–∑–≤–ª–µ–∫–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è –∫–æ—É—á–∏–Ω–≥–æ–≤–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
### –¢–µ—Ö–Ω–∏–∫–∏ –∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è
- **–ù–∞–∑–≤–∞–Ω–∏–µ**: –û–ø–∏—Å–∞–Ω–∏–µ –∏ –ø–æ—à–∞–≥–æ–≤–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è

### –í–æ–ø—Ä–æ—Å—ã –¥–ª—è –∫–æ—É—á–∞
[–ú–æ—â–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–∞]

### –§—Ä–µ–π–º–≤–æ—Ä–∫–∏ –¥–ª—è —Å–µ—Å—Å–∏–π
[–°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∫–æ—É—á–∏–Ω–≥–æ–≤—ã—Ö —Å–µ—Å—Å–∏–π]

### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
[–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–æ–≤–µ—Ç—ã –ø–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é]
""",
            
            "questions": """–ó–ê–î–ê–ß–ê: –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
### –í–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è
[5 –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞]

### –í–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è  
[5 –≤–æ–ø—Ä–æ—Å–æ–≤ –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –≤ –ø—Ä–∞–∫—Ç–∏–∫–µ]

### –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å–∞–º–æ–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
[5 –≥–ª—É–±–æ–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ª–∏—á–Ω–æ–π —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏]

### –í–æ–ø—Ä–æ—Å—ã –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è
[3 –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –≥—Ä—É–ø–ø–æ–≤–æ–π –¥–∏—Å–∫—É—Å—Å–∏–∏]
""",
            
            "full_analysis": """–ó–ê–î–ê–ß–ê: –ü—Ä–æ–≤–µ–¥–∏ –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–∞

–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:

## 1. –°–ê–ú–ú–ê–†–ò
[–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤ 3-5 –∞–±–∑–∞—Ü–∞—Ö]

## 2. –ö–õ–Æ–ß–ï–í–´–ï –ö–û–ù–¶–ï–ü–¶–ò–ò
| –ö–æ–Ω—Ü–µ–ø—Ü–∏—è | –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ | –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ |
|-----------|-------------|------------|
[–¢–∞–±–ª–∏—Ü–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π]

## 3. –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ –ò –¢–ï–•–ù–ò–ö–ò
[–°–ø–∏—Å–æ–∫ —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏]

## 4. –ú–û–©–ù–´–ï –í–û–ü–†–û–°–´
[–í–æ–ø—Ä–æ—Å—ã –∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –∏–ª–∏ –ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª—É]

## 5. –°–í–Ø–ó–ò –ò –ò–ù–°–ê–ô–¢–´
[–ö–∞–∫ —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ–º–∞–º–∏ –∫–æ—É—á–∏–Ω–≥–∞]

## 6. –ü–†–ê–ö–¢–ò–ß–ï–°–ö–û–ï –ü–†–ò–ú–ï–ù–ï–ù–ò–ï
[–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è]

## 7. –¢–ï–ì–ò –î–õ–Ø –ü–û–ò–°–ö–ê
[5-10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤/—Ç–µ–≥–æ–≤]
"""
        }
        
        return base_prompt + mode_prompts.get(mode, mode_prompts["summary"]) + f"\n\n{context}"
    
    def process_chunk(self, chunk: Dict, mode: str = "full_analysis",
                      additional_context: str = "") -> Dict:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —á–∞–Ω–∫ —á–µ—Ä–µ–∑ Claude API.

        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        system_prompt = self.get_system_prompt(mode, additional_context)

        user_message = f"""–û–±—Ä–∞–±–æ—Ç–∞–π —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —É—á–µ–±–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞:

---
{chunk['text']}
---

–°—Ç—Ä–∞–Ω–∏—Ü—ã: {chunk.get('page_range', 'N/A')}
–§—Ä–∞–≥–º–µ–Ω—Ç: {chunk['chunk_id']}
"""

        try:
            # Rate limiting –ø–µ—Ä–µ–¥ API –≤—ã–∑–æ–≤–æ–º
            rate_limiter.wait()
            logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {chunk['chunk_id']}, —Å—Ç—Ä–∞–Ω–∏—Ü—ã {chunk.get('page_range', 'N/A')}")

            response = self.client.messages.create(
                model=self.config["model"],
                max_tokens=self.config["max_output_tokens"],
                system=system_prompt,
                messages=[{"role": "user", "content": user_message}]
            )
            
            result = {
                "chunk_id": chunk["chunk_id"],
                "page_range": chunk.get("page_range", "N/A"),
                "mode": mode,
                "processed_at": datetime.now().isoformat(),
                "response": response.content[0].text,
                "usage": {
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens
                },
                "status": "success"
            }
            
        except Exception as e:
            result = {
                "chunk_id": chunk["chunk_id"],
                "page_range": chunk.get("page_range", "N/A"),
                "mode": mode,
                "processed_at": datetime.now().isoformat(),
                "error": str(e),
                "status": "error"
            }
        
        return result
    
    def process_pdf(self, pdf_path: str, mode: str = "full_analysis",
                    save_intermediate: bool = True, resume: bool = True) -> Dict:
        """
        –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF.
        
        Args:
            pdf_path: –ü—É—Ç—å –∫ PDF
            mode: –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            save_intermediate: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            resume: –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            
        Returns:
            Dict: –ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        pdf_path = Path(pdf_path)
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è —ç—Ç–æ–≥–æ PDF
        pdf_hash = hashlib.md5(pdf_path.name.encode()).hexdigest()[:8]
        session_id = f"{pdf_path.stem}_{pdf_hash}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
        progress_file = self.chunks_dir / f"{session_id}_progress.json"
        
        if resume and progress_file.exists():
            with open(progress_file) as f:
                progress = json.load(f)
            print(f"üìÇ –ü—Ä–æ–¥–æ–ª–∂–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å —á–∞–Ω–∫–∞ {progress['last_completed'] + 1}")
            start_chunk = progress["last_completed"] + 1
            chunks = progress["chunks"]
            results = progress["results"]
            metadata = progress["metadata"]
        else:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text, metadata = self.extract_text_from_pdf(pdf_path)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç
            raw_text_path = self.output_dir / f"{session_id}_raw.txt"
            with open(raw_text_path, "w", encoding="utf-8") as f:
                f.write(text)
            print(f"üíæ –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {raw_text_path}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.smart_chunk_text(text)
            results = []
            start_chunk = 0
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏
        total_chunks = len(chunks)
        
        print(f"\nüöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {total_chunks} —á–∞–Ω–∫–æ–≤ –≤ —Ä–µ–∂–∏–º–µ '{mode}'")
        print("=" * 50)
        
        for i, chunk in enumerate(chunks[start_chunk:], start_chunk + 1):
            print(f"\nüìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —á–∞–Ω–∫ {i}/{total_chunks} (—Å—Ç—Ä. {chunk.get('page_range', 'N/A')})...")
            
            result = self.process_chunk(chunk, mode)
            results.append(result)
            
            if result["status"] == "success":
                print(f"   ‚úÖ –ì–æ—Ç–æ–≤–æ ({result['usage']['input_tokens']}‚Üí{result['usage']['output_tokens']} —Ç–æ–∫–µ–Ω–æ–≤)")
            else:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {result.get('error', 'Unknown')}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if save_intermediate:
                progress = {
                    "session_id": session_id,
                    "pdf_path": str(pdf_path),
                    "mode": mode,
                    "last_completed": i,
                    "total_chunks": total_chunks,
                    "chunks": chunks,
                    "results": results,
                    "metadata": metadata,
                    "updated_at": datetime.now().isoformat()
                }
                with open(progress_file, "w", encoding="utf-8") as f:
                    json.dump(progress, f, ensure_ascii=False, indent=2)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        final_result = {
            "session_id": session_id,
            "source_pdf": str(pdf_path),
            "metadata": metadata,
            "mode": mode,
            "total_chunks": total_chunks,
            "successful_chunks": sum(1 for r in results if r["status"] == "success"),
            "processed_at": datetime.now().isoformat(),
            "results": results,
            "total_tokens": {
                "input": sum(r.get("usage", {}).get("input_tokens", 0) for r in results),
                "output": sum(r.get("usage", {}).get("output_tokens", 0) for r in results)
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        output_path = self.output_dir / f"{session_id}_processed.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–µ–º —á–∏—Ç–∞–µ–º—ã–π Markdown
        markdown_path = self.output_dir / f"{session_id}_processed.md"
        self._save_as_markdown(final_result, markdown_path)
        
        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        if progress_file.exists():
            progress_file.unlink()
        
        print("\n" + "=" * 50)
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {final_result['successful_chunks']}/{total_chunks}")
        print(f"   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {final_result['total_tokens']['input']:,} –≤—Ö–æ–¥ / {final_result['total_tokens']['output']:,} –≤—ã—Ö–æ–¥")
        print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   - JSON: {output_path}")
        print(f"   - Markdown: {markdown_path}")
        
        return final_result
    
    def _save_as_markdown(self, result: Dict, output_path: Path):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —á–∏—Ç–∞–µ–º–æ–º Markdown —Ñ–æ—Ä–º–∞—Ç–µ."""
        
        md_content = f"""# –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª: {result['source_pdf']}

**–î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {result['processed_at']}
**–†–µ–∂–∏–º:** {result['mode']}
**–°—Ç—Ä–∞–Ω–∏—Ü –≤ PDF:** {result['metadata'].get('pages', 'N/A')}
**–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤:** {result['successful_chunks']}/{result['total_chunks']}

---

"""
        
        for r in result["results"]:
            if r["status"] == "success":
                md_content += f"""
## –§—Ä–∞–≥–º–µ–Ω—Ç {r['chunk_id']} (—Å—Ç—Ä. {r['page_range']})

{r['response']}

---
"""
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(md_content)
    
    def build_knowledge_base(self, processed_files: List[str] = None) -> Dict:
        """
        –°—Ç—Ä–æ–∏—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.
        
        Args:
            processed_files: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ JSON —Ñ–∞–π–ª–∞–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
                           –ï—Å–ª–∏ None, –±–µ—Ä–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ output/
        
        Returns:
            Dict: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π
        """
        if processed_files is None:
            processed_files = list(self.output_dir.glob("*_processed.json"))
        
        knowledge_base = {
            "created_at": datetime.now().isoformat(),
            "sources": [],
            "concepts": [],
            "tools": [],
            "questions": [],
            "summaries": []
        }
        
        for file_path in processed_files:
            with open(file_path, encoding="utf-8") as f:
                data = json.load(f)
            
            source_info = {
                "filename": Path(data["source_pdf"]).name,
                "session_id": data["session_id"],
                "pages": data["metadata"].get("pages", 0)
            }
            knowledge_base["sources"].append(source_info)
            
            # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            for result in data["results"]:
                if result["status"] == "success":
                    response = result["response"]
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç –∫–∞–∫ —Å–∞–º–º–∞—Ä–∏
                    knowledge_base["summaries"].append({
                        "source": source_info["filename"],
                        "chunk_id": result["chunk_id"],
                        "page_range": result["page_range"],
                        "content": response
                    })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        kb_path = self.knowledge_dir / "knowledge_base.json"
        with open(kb_path, "w", encoding="utf-8") as f:
            json.dump(knowledge_base, f, ensure_ascii=False, indent=2)
        
        print(f"üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞: {kb_path}")
        print(f"   - –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(knowledge_base['sources'])}")
        print(f"   - –°–∞–º–º–∞—Ä–∏: {len(knowledge_base['summaries'])}")
        
        return knowledge_base
    
    def ask_question(self, question: str, knowledge_base_path: str = None,
                     max_context_chunks: int = 5) -> str:
        """
        –ó–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            knowledge_base_path: –ü—É—Ç—å –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
            max_context_chunks: –ú–∞–∫—Å–∏–º—É–º —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            str: –û—Ç–≤–µ—Ç –æ—Ç Claude
        """
        if knowledge_base_path is None:
            knowledge_base_path = self.knowledge_dir / "knowledge_base.json"
        
        with open(knowledge_base_path, encoding="utf-8") as f:
            kb = json.load(f)
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å embeddings)
        relevant_chunks = []
        question_lower = question.lower()
        
        for summary in kb["summaries"]:
            content_lower = summary["content"].lower()
            # –ü—Ä–æ—Å—Ç–æ–π —Å–∫–æ—Ä–∏–Ω–≥ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é —Å–ª–æ–≤
            score = sum(1 for word in question_lower.split() if word in content_lower)
            if score > 0:
                relevant_chunks.append((score, summary))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º —Ç–æ–ø
        relevant_chunks.sort(key=lambda x: x[0], reverse=True)
        context_chunks = [c[1] for c in relevant_chunks[:max_context_chunks]]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = "\n\n---\n\n".join([
            f"–ò—Å—Ç–æ—á–Ω–∏–∫: {c['source']}, —Å—Ç—Ä. {c['page_range']}\n{c['content']}"
            for c in context_chunks
        ])
        
        system_prompt = """–¢—ã - AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–æ–≥—Ä–∞–º–º—ã Executive Coaching & Mentoring –≤ –°–ö–û–õ–ö–û–í–û.
–û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
–°—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        
        user_message = f"""–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –£–ß–ï–ë–ù–´–• –ú–ê–¢–ï–†–ò–ê–õ–û–í:

{context}

---

–í–û–ü–†–û–°: {question}"""
        
        response = self.client.messages.create(
            model=self.config["model"],
            max_tokens=self.config["max_output_tokens"],
            system=system_prompt,
            messages=[{"role": "user", "content": user_message}]
        )
        
        return response.content[0].text


def main():
    """CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å."""
    parser = argparse.ArgumentParser(
        description="PDF AI Processor –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ —á–µ—Ä–µ–∑ Claude API"
    )
    
    subparsers = parser.add_subparsers(dest="command", help="–ö–æ–º–∞–Ω–¥—ã")
    
    # –ö–æ–º–∞–Ω–¥–∞ process
    process_parser = subparsers.add_parser("process", help="–û–±—Ä–∞–±–æ—Ç–∞—Ç—å PDF")
    process_parser.add_argument("pdf_path", help="–ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É")
    process_parser.add_argument(
        "--mode", "-m",
        choices=["summary", "key_concepts", "coaching_tools", "questions", "full_analysis"],
        default="full_analysis",
        help="–†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ (default: full_analysis)"
    )
    process_parser.add_argument(
        "--no-resume",
        action="store_true",
        help="–ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–Ω–æ–≤–æ, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å"
    )
    
    # –ö–æ–º–∞–Ω–¥–∞ ask
    ask_parser = subparsers.add_parser("ask", help="–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π")
    ask_parser.add_argument("question", help="–í–æ–ø—Ä–æ—Å")
    
    # –ö–æ–º–∞–Ω–¥–∞ build-kb
    kb_parser = subparsers.add_parser("build-kb", help="–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
    
    # –û–±—â–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    parser.add_argument(
        "--api-key", "-k",
        help="API –∫–ª—é—á Anthropic (–∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ANTHROPIC_API_KEY)"
    )
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        processor = PDFAIProcessor(api_key=args.api_key)
        
        if args.command == "process":
            processor.process_pdf(
                args.pdf_path,
                mode=args.mode,
                resume=not args.no_resume
            )
        
        elif args.command == "ask":
            answer = processor.ask_question(args.question)
            print("\n" + "=" * 50)
            print("–û–¢–í–ï–¢:")
            print("=" * 50)
            print(answer)
        
        elif args.command == "build-kb":
            processor.build_knowledge_base()
    
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
