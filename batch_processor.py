#!/usr/bin/env python3
"""
Batch Processor - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö PDF –≤ –ø–∞–ø–∫–µ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python batch_processor.py /–ø—É—Ç—å/–∫/–ø–∞–ø–∫–µ/—Å/pdf
    python batch_processor.py /–ø—É—Ç—å/–∫/–ø–∞–ø–∫–µ/—Å/pdf --mode summary
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime

from pdf_ai_processor import PDFAIProcessor


def process_all_pdfs(folder_path: str, mode: str = "full_analysis", 
                     api_key: str = None, skip_processed: bool = True):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ PDF –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ.
    
    Args:
        folder_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å PDF
        mode: –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
        api_key: API –∫–ª—é—á
        skip_processed: –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    """
    folder = Path(folder_path)
    if not folder.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder}")
        return
    
    pdf_files = list(folder.glob("*.pdf")) + list(folder.glob("*.PDF"))
    
    if not pdf_files:
        print(f"‚ùå PDF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤: {folder}")
        return
    
    print(f"\nüìÇ –ù–∞–π–¥–µ–Ω–æ {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤ –≤ {folder}")
    print("=" * 60)
    
    processor = PDFAIProcessor(api_key=api_key)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
    processed_files = set(f.stem.split("_")[0] for f in processor.output_dir.glob("*_processed.json"))
    
    results = []
    errors = []
    
    for i, pdf_file in enumerate(pdf_files, 1):
        print(f"\n[{i}/{len(pdf_files)}] üìÑ {pdf_file.name}")
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
        if skip_processed and pdf_file.stem in [p.split("_")[0] for p in processed_files]:
            print("   ‚è≠Ô∏è –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞—é")
            continue
        
        try:
            result = processor.process_pdf(str(pdf_file), mode=mode)
            results.append({
                "file": pdf_file.name,
                "status": "success",
                "chunks": result["successful_chunks"],
                "tokens": result["total_tokens"]
            })
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
            errors.append({
                "file": pdf_file.name,
                "error": str(e)
            })
    
    # –°—Ç—Ä–æ–∏–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
    print("\n" + "=" * 60)
    print("üìö –°–æ–∑–¥–∞—é –µ–¥–∏–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π...")
    processor.build_knowledge_base()
    
    # –û—Ç—á–µ—Ç
    print("\n" + "=" * 60)
    print("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("=" * 60)
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)}")
    print(f"‚ùå –û—à–∏–±–æ–∫: {len(errors)}")
    
    if results:
        total_tokens = sum(r["tokens"]["input"] + r["tokens"]["output"] for r in results)
        print(f"üìà –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens:,}")
        print(f"üí∞ –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${total_tokens * 0.000003:.2f} (Sonnet)")
    
    if errors:
        print("\n‚ö†Ô∏è –§–∞–π–ª—ã —Å –æ—à–∏–±–∫–∞–º–∏:")
        for e in errors:
            print(f"   - {e['file']}: {e['error']}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    report = {
        "processed_at": datetime.now().isoformat(),
        "folder": str(folder),
        "mode": mode,
        "successful": results,
        "errors": errors
    }
    
    report_path = processor.output_dir / "batch_report.json"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nüìÅ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")


def create_structured_knowledge_base(api_key: str = None):
    """
    –°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–µ–π.
    """
    processor = PDFAIProcessor(api_key=api_key)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    processed_files = list(processor.output_dir.glob("*_processed.json"))
    
    if not processed_files:
        print("‚ùå –ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        return
    
    print(f"üìö –°–æ–∑–¥–∞—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∞–∑—É –∏–∑ {len(processed_files)} —Ñ–∞–π–ª–æ–≤...")
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
    structured_kb = {
        "meta": {
            "created_at": datetime.now().isoformat(),
            "program": "SKOLKOVO Executive Coaching & Mentoring",
            "total_sources": len(processed_files)
        },
        "categories": {
            "coaching_fundamentals": {
                "name": "–û—Å–Ω–æ–≤—ã –∫–æ—É—á–∏–Ω–≥–∞",
                "items": []
            },
            "psychological_concepts": {
                "name": "–ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏",
                "items": []
            },
            "team_development": {
                "name": "–†–∞–∑–≤–∏—Ç–∏–µ –∫–æ–º–∞–Ω–¥",
                "items": []
            },
            "emotional_intelligence": {
                "name": "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
                "items": []
            },
            "leadership": {
                "name": "–õ–∏–¥–µ—Ä—Å—Ç–≤–æ",
                "items": []
            },
            "tools_and_techniques": {
                "name": "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–∏–∫–∏",
                "items": []
            },
            "models_and_frameworks": {
                "name": "–ú–æ–¥–µ–ª–∏ –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏",
                "items": []
            },
            "questions_bank": {
                "name": "–ë–∞–Ω–∫ –≤–æ–ø—Ä–æ—Å–æ–≤",
                "items": []
            }
        },
        "sources": [],
        "index": {}  # –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    }
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
    category_keywords = {
        "coaching_fundamentals": ["–∫–æ—É—á–∏–Ω–≥", "–∫–æ—É—á", "–∫–æ–Ω—Ç—Ä–∞–∫—Ç", "—Å–µ—Å—Å–∏—è", "–∫–ª–∏–µ–Ω—Ç"],
        "psychological_concepts": ["–ø—Å–∏—Ö–æ–ª–æ–≥", "–±–µ—Å—Å–æ–∑–Ω–∞—Ç–µ–ª—å–Ω", "–∑–∞—â–∏—Ç", "–ø–µ—Ä–µ–Ω–æ—Å", "–ø—Ä–æ–µ–∫—Ü"],
        "team_development": ["–∫–æ–º–∞–Ω–¥", "–≥—Ä—É–ø–ø–æ–≤", "–¥–∏–Ω–∞–º–∏–∫", "—Ä–æ–ª—å", "–∫–æ–Ω—Ñ–ª–∏–∫—Ç"],
        "emotional_intelligence": ["—ç–º–æ—Ü", "—á—É–≤—Å—Ç–≤", "—ç–º–ø–∞—Ç–∏—è", "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å"],
        "leadership": ["–ª–∏–¥–µ—Ä", "—Ä—É–∫–æ–≤–æ–¥", "—É–ø—Ä–∞–≤–ª–µ–Ω", "–≤–ª–∏—è–Ω–∏"],
        "tools_and_techniques": ["—Ç–µ—Ö–Ω–∏–∫", "—É–ø—Ä–∞–∂–Ω–µ–Ω", "–º–µ—Ç–æ–¥", "–ø—Ä–∞–∫—Ç–∏–∫", "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç"],
        "models_and_frameworks": ["–º–æ–¥–µ–ª—å", "—Ñ—Ä–µ–π–º–≤–æ—Ä–∫", "–∫–æ–Ω—Ü–µ–ø—Ü", "—Ç–µ–æ—Ä–∏", "–ø–æ–¥—Ö–æ–¥"]
    }
    
    for file_path in processed_files:
        with open(file_path, encoding="utf-8") as f:
            data = json.load(f)
        
        source_info = {
            "filename": Path(data["source_pdf"]).name,
            "session_id": data["session_id"],
            "pages": data["metadata"].get("pages", 0),
            "processed_at": data["processed_at"]
        }
        structured_kb["sources"].append(source_info)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        for result in data["results"]:
            if result["status"] != "success":
                continue
            
            content = result["response"]
            content_lower = content.lower()
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ–º
            for category, keywords in category_keywords.items():
                if any(kw in content_lower for kw in keywords):
                    item = {
                        "source": source_info["filename"],
                        "page_range": result["page_range"],
                        "content": content,
                        "chunk_id": result["chunk_id"]
                    }
                    structured_kb["categories"][category]["items"].append(item)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã
            if "?" in content:
                questions = [
                    line.strip() for line in content.split("\n")
                    if "?" in line and len(line) > 20
                ]
                for q in questions[:10]:  # –ú–∞–∫—Å–∏–º—É–º 10 –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ —á–∞–Ω–∫
                    structured_kb["categories"]["questions_bank"]["items"].append({
                        "question": q,
                        "source": source_info["filename"],
                        "page_range": result["page_range"]
                    })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å
            words = set(content_lower.split())
            for word in words:
                if len(word) > 4:  # –¢–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 4 —Å–∏–º–≤–æ–ª–æ–≤
                    if word not in structured_kb["index"]:
                        structured_kb["index"][word] = []
                    structured_kb["index"][word].append({
                        "source": source_info["filename"],
                        "chunk_id": result["chunk_id"]
                    })
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:")
    for cat_id, cat_data in structured_kb["categories"].items():
        print(f"   {cat_data['name']}: {len(cat_data['items'])} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    kb_path = processor.knowledge_dir / "structured_knowledge_base.json"
    with open(kb_path, "w", encoding="utf-8") as f:
        json.dump(structured_kb, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {kb_path}")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–∫–∂–µ Markdown –≤–µ—Ä—Å–∏—é –¥–ª—è —á—Ç–µ–Ω–∏—è
    create_markdown_knowledge_base(structured_kb, processor.knowledge_dir)
    
    return structured_kb


def create_markdown_knowledge_base(kb: dict, output_dir: Path):
    """–°–æ–∑–¥–∞–µ—Ç —á–∏—Ç–∞–µ–º—É—é Markdown –≤–µ—Ä—Å–∏—é –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."""
    
    md_content = f"""# üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π SKOLKOVO Executive Coaching

**–°–æ–∑–¥–∞–Ω–æ:** {kb['meta']['created_at']}  
**–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤:** {kb['meta']['total_sources']}

---

## üìñ –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

"""
    
    # TOC
    for cat_id, cat_data in kb["categories"].items():
        if cat_data["items"]:
            md_content += f"- [{cat_data['name']}](#{cat_id}) ({len(cat_data['items'])})\n"
    
    md_content += "\n---\n\n"
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
    for cat_id, cat_data in kb["categories"].items():
        if not cat_data["items"]:
            continue
        
        md_content += f"## {cat_data['name']} {{#{cat_id}}}\n\n"
        
        if cat_id == "questions_bank":
            # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ—Å–æ–±—ã–π —Ñ–æ—Ä–º–∞—Ç
            for item in cat_data["items"][:50]:  # –ü–µ—Ä–≤—ã–µ 50
                md_content += f"- {item['question']}\n"
                md_content += f"  *[{item['source']}, —Å—Ç—Ä. {item['page_range']}]*\n\n"
        else:
            for i, item in enumerate(cat_data["items"][:20], 1):  # –ü–µ—Ä–≤—ã–µ 20
                md_content += f"### {i}. [{item['source']}, —Å—Ç—Ä. {item['page_range']}]\n\n"
                md_content += f"{item['content'][:1000]}...\n\n"
                md_content += "---\n\n"
    
    md_path = output_dir / "knowledge_base.md"
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(md_content)
    
    print(f"üìÑ Markdown –≤–µ—Ä—Å–∏—è: {md_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"
    )
    
    subparsers = parser.add_subparsers(dest="command")
    
    # –ö–æ–º–∞–Ω–¥–∞ process
    process_parser = subparsers.add_parser("process", help="–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ PDF –≤ –ø–∞–ø–∫–µ")
    process_parser.add_argument("folder", help="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å PDF")
    process_parser.add_argument("--mode", "-m", default="full_analysis",
                               choices=["summary", "key_concepts", "coaching_tools", 
                                       "questions", "full_analysis"])
    process_parser.add_argument("--reprocess", action="store_true",
                               help="–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã")
    
    # –ö–æ–º–∞–Ω–¥–∞ structure
    structure_parser = subparsers.add_parser("structure", 
                                            help="–°–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
    
    # –û–±—â–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    parser.add_argument("--api-key", "-k", help="API –∫–ª—é—á Anthropic")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    if args.command == "process":
        process_all_pdfs(
            args.folder,
            mode=args.mode,
            api_key=args.api_key,
            skip_processed=not args.reprocess
        )
    
    elif args.command == "structure":
        create_structured_knowledge_base(api_key=args.api_key)


if __name__ == "__main__":
    main()
